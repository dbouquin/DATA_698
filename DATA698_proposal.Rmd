---
title: 'A long short-term memory recurrent neural network for generating astrophysics-specific language and assessment using tf-idf, cosine similarity, and a vector space model'
subtitle: "Capstone Proposal: CUNY School of Professional Studies, MS in Data Analytics"
author: "Daina Bouquin"
date: "September 16, 2017"
output: pdf_document
---

### 1. Introduction   
In 1989, a very simple definition of an "artificial neural network" (ANN) was presented by Maureen Caudill, an expert on artificial intellegence. Caudill wrote that an artifical neural network is "...a computing system made up of a number of simple, highly interconnected processing elements, which process information by their dynamic state response to external inputs" [1]. With this definition in mind, what I propose herein is a small study on the application of a particular type of recurant ANN, long short-term memory (LSTM), to generate domain-specific text in various subdisciplines of astronomy and astrophysics. Subsequent to the training and sampling of astronomy and astrophysics related text, I will apply measures including term frequency-inverse document frequency (tf-idf) and cosine similarity, along with a vector space model to assess the degree to which the text generated by the ANN accurately reflects the text used to train the system. In conducting this study I will be applying this approach to a dataset that has not yet been used to test the capacity for this type of system to "learn". I will also be applying an LSTM ANN to in a jargon-heavy scientific domain, which is an area of limited research.

### 2. Data Source
The SAO/NASA Astrophysics Data System (ADS) [3], is an online database of over eight million astronomy and physics papers from both peer reviewed and non-peer reviewed sources. The ADS is a highly used resource in the Astronomy and Physics communities and has many levels of indexing. The ADS API makes it possible to query this valuable resource to better understand authorship and publishing behavior in these fields among many other applications. The ADS allows for full-text searching, and in the near future, will be fully incorporating Unified Astronomy Thesaurus keywords into their indexing schema. The ADS is managed by the Smithsonian Astrophysical Observatory at the Harvardâ€“Smithsonian Center for Astrophysics with funding from NASA.

### 3. Background   
Artificial neural networks can be understood as being organized into "layers" with layers being made up of a number of interconnected "nodes" which each contain an "activation function". Data are passed into the "input layer" of the system, which communicates to "hidden layers" where the actual processing is done via a system of weighted "connections". Subsequently, the hidden layers connect with an "output layer" where the answer is output [2]. Since the creation of ANNs, these layered connections of nodes have been implementated in many different ways. Each implementation has been designed to meet different needs and experiments. Perhaps the most recognizable application of ANNs in popular culture is Google's "Deep Dream", a computer vision program that uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia [4]. In text-based applications of ANNs though, recurant neural networks have become favored over convolutional systems. For instance, Twitter bots like "Deep Drumpf" [6] have been used to demonstrate the powerful effectiveness of recurrant ANNs by emulating the linguistical style and syntax of individual people. Andrej Karpathy, Tesla Motors' leader on artifical intelligence reserach, has stated that some of the reasoning behind the "unreasonable effectiveness of recurrant neural networks" is that with convolutional systems, "their API is too constrained" in that they accept only a fixed-sized vector as input and subsequently produce a fixed-sized vector as output using a fixed amount of computational steps. Unlike these convolutional ANNs, recurant ANNs allow us to operate over sequences of vectors: sequences in the input, output, or even both [5].

To further define and justify the approach to be used for the herein described experiment, it is necessary to define the specific recurant ANN I will be employing: long short-term memory (LTSM). LTSM ANNs are "capable of learning long-term dependencies" [7] and do not start from scratch whenever they are presented with new data. LTSMs were introduced by Hochreiter and Schmidhuber in 1997 [8] and were refined iteratively following their work. The LTSM approach to recurant ANNs is ideal for the type of text-based analysis I will be conducting because by using LTSM you can train the system on a large corpus of text and it will "learn" to generate text like it one character at a time. This, along with LTSM's relative insensitivity to "gap length" gives an advantage to LSTM over alternative recurant neural networks [9].

### 5. Test Cases
A description of the question you are examining and an exploration of the claims.
List the specific question(s) that you are exploring.
Explain how these research questions are related to the larger issues raised in the
introduction.

Will start with titles. Move up to abstracts should there be enough time.

### Hypothesis
Describe what specific claim, hypothesis, and/or model of psycholinguistics you will
evaluate with these questions.
Hypthothize that some fields will be more easy to reporduce based on the scope of the domain. The diversity of terms used. Show which subdomain has the most unique words-- that field will be more challenging to reproduce. More data will increase the similarity between generated text and real text.

### Assessment

Examples of assessment protocols in use (digital libraries)
No example of this sort of application in astronomy though, hense need to look at an application here.

### Tools
There are many computational tools to help analysts implement LSTM recurant ANNs. I am currently exploring the following options to implement my proposed study: Theano, Torch, TensorFlow, and a straight Python/numpy approach. Each of these potential tools has pros and cons associated with their use. I am limited in the amount of processing power I have available to me and this may in the end be the deciding factor in chosing a tool and scale for the implementation of the ANN. I will use Python and the following modules for the assessment portion of the project: Pandas, Numpy, Sklearn.feature_extraction.text, Skylearn.metricspairwise, and math. I will use the ADS API to obtain the datasets that will be used for training, sampling, and testing the system.

### 6. Known Limitations and Implications
This will be interesting examination of the nature of astronomy text

### 7. References
[1] "Neural Network Primer: Part I" by Maureen Caudill, AI Expert, Feb. 1989    
[2] http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html     
[3] https://ui.adsabs.harvard.edu/
[4] https://arxiv.org/abs/1409.4842
[5] https://karpathy.github.io/2015/05/21/rnn-effectiveness/
[6] http://www.deepdrumpf2016.com/about.html
[7] https://colah.github.io/posts/2015-08-Understanding-LSTMs/
[8] http://www.bioinf.jku.at/publications/older/2604.pdf
[9] https://en.wikipedia.org/wiki/Long_short-term_memory
